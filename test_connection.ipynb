{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5078df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "\n",
    "project_root = os.path.abspath(\".\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6513c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connectors.database import DatabaseConnector\n",
    "from config.settings import DB_URL\n",
    "\n",
    "connector = DatabaseConnector(DB_URL)\n",
    "engine = connector.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b930c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load.to_db import DataBaseLoader\n",
    "from load.to_file import FileLoader\n",
    "from extract.from_db import DatabaseExtractor\n",
    "from extract.from_file import FileExtractor\n",
    "from extract.from_API import ExchangeRateFetcher\n",
    "from transform.add_quality import ADDQuality\n",
    "from validate.quality_checks import DataValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf09a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:07:03 => ✅ Extracted 1445 rows from customers.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 7 rows from categories.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 334 rows from products.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 1615 rows from orders.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 10 rows from staffs.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 4764 rows from order_items.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 9 rows from brands.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 3 rows from stores.csv in source\n",
      "2025-04-16 15:07:03 => ✅ Extracted 939 rows from stocks.csv in source\n",
      "2025-04-16 15:07:04 =>   Fetched 169 exchange rates from openexchangerates\n"
     ]
    }
   ],
   "source": [
    "csv_extractor = FileExtractor(\"/Users/mohamedmoaaz/Desktop/usecase\")\n",
    "source_data_frames = csv_extractor.extract_from_csv(\"source\")\n",
    "exchange_rate_extractor = ExchangeRateFetcher(\"https://openexchangerates.org/api/latest.json?app_id=a92f8bad8e044bc79949a676886da2c8\")\n",
    "exchange_rate_df = exchange_rate_extractor.fetch_exchange_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0420705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:07:04 => ✅ Loaded 1445 rows into customers.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 7 rows into categories.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 334 rows into products.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 1615 rows into orders in PostgreSQL\n",
      "2025-04-16 15:07:04 => ✅ Loaded 10 rows into staffs.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 4764 rows into order_items in PostgreSQL\n",
      "2025-04-16 15:07:04 => ✅ Loaded 9 rows into brands.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 3 rows into stores.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 939 rows into stocks.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Loaded 169 rows into exchange_rates.csv in datalake\n"
     ]
    }
   ],
   "source": [
    "postgres_loader = DataBaseLoader(engine)\n",
    "csv_loader = FileLoader(\"/Users/mohamedmoaaz/Desktop/usecase\")\n",
    "for file_name in source_data_frames.keys():\n",
    "    if file_name == \"orders.csv\" or file_name == \"order_items.csv\":\n",
    "        table_name = file_name.replace(\".csv\", \"\")\n",
    "        postgres_loader.load_to_postgres(source_data_frames[file_name], table_name)\n",
    "    else:\n",
    "        csv_loader.load_to_csv(source_data_frames[file_name], \"datalake\", file_name)\n",
    "csv_loader.load_to_csv(exchange_rate_df, \"datalake\", \"exchange_rates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c902704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:07:04 => ✅ Extracted 1445 rows from customers.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 7 rows from categories.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 334 rows from products.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 169 rows from exchange_rates.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 10 rows from staffs.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 9 rows from brands.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 3 rows from stores.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 939 rows from stocks.csv in datalake\n",
      "2025-04-16 15:07:04 => ✅ Extracted 4764 rows from PostgreSQL\n",
      "2025-04-16 15:07:04 => ✅ Extracted 1615 rows from PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "csv_extractor = FileExtractor(\"/Users/mohamedmoaaz/Desktop/usecase\")\n",
    "data_lake_data = csv_extractor.extract_from_csv(\"datalake\")\n",
    "\n",
    "postgres_extractor = DatabaseExtractor(DB_URL)\n",
    "query1 = \"SELECT * FROM order_items\"\n",
    "order_data = postgres_extractor.extract_from_postgres(query1, engine)\n",
    "\n",
    "query2 = \"SELECT * FROM orders\"\n",
    "order_items_data = postgres_extractor.extract_from_postgres(query2, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614dd566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 1445 rows into customers.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 7 rows into categories.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 334 rows into products.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 169 rows into exchange_rates.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 10 rows into staffs.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 9 rows into brands.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 3 rows into stores.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 939 rows into stocks.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 4764 rows into stocks.csv in landing\n",
      "2025-04-16 15:07:05 =>    Added source and extraction date to data\n",
      "2025-04-16 15:07:05 => ✅ Loaded 1615 rows into stocks.csv in landing\n"
     ]
    }
   ],
   "source": [
    "data_quality = ADDQuality()\n",
    "csv_loader = FileLoader(\"/Users/mohamedmoaaz/Desktop/usecase\")\n",
    "for file_name in data_lake_data.keys():\n",
    "    qualified_data = data_quality.add_source_and_date(data_lake_data[file_name], \"data lake\")\n",
    "    csv_loader.load_to_csv(qualified_data, \"landing\", file_name)\n",
    "qualified_data = data_quality.add_source_and_date(order_data, \"postgres db\")\n",
    "csv_loader.load_to_csv(qualified_data, \"landing\", file_name)\n",
    "qualified_data = data_quality.add_source_and_date(order_items_data, \"postgres db\")\n",
    "csv_loader.load_to_csv(qualified_data, \"landing\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4063fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:07:06 => ✅ Extracted 1445 rows from customers.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 7 rows from categories.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 334 rows from products.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 169 rows from exchange_rates.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 10 rows from staffs.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 9 rows from brands.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 3 rows from stores.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Extracted 1615 rows from stocks.csv in landing\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 1445 rows into customers.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 7 rows into categories.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 13 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 321 rows into products.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 169 rows into exchange_rates.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 10 rows into staffs.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 9 rows into brands.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Loaded 3 rows into stores.csv in staging\n",
      "2025-04-16 15:07:06 => ✅ Handled null values in columns: None with fill value: 0\n",
      "2025-04-16 15:07:06 => ✅ Removed 0 duplicate rows\n",
      "2025-04-16 15:07:06 => ✅ Validated data types and handled errors\n",
      "2025-04-16 15:07:06 => ✅ Loaded 1604 rows into stocks.csv in staging\n"
     ]
    }
   ],
   "source": [
    "csv_extractor = FileExtractor(\"/Users/mohamedmoaaz/Desktop/usecase\")\n",
    "landing_data = csv_extractor.extract_from_csv(\"landing\")\n",
    "for file_name, df in landing_data.items():\n",
    "    data_validator = DataValidator(df)\n",
    "    data_validator.handle_nulls()\n",
    "    data_validator.remove_duplicates()\n",
    "    data_validator.validate_data()\n",
    "    csv_loader.load_to_csv(df, \"staging\", file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
